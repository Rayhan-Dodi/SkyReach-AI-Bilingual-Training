{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sky Reach AI Bilingual Training\n",
    "\n",
    "This notebook demonstrates how to train the Sky Reach AI bilingual model with Bangla and English data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install required packages\n",
    "\n",
    "- Install necessary libraries for transformers, datasets, PyTorch, and sentencepiece tokenizer support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [48 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n",
      "          main()\n",
      "          ~~~~^^\n",
      "        File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n",
      "          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "                                   ~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python313\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 143, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "        File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-build-env-ola19u55\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 331, in get_requires_for_build_wheel\n",
      "          return self._get_build_requires(config_settings, requirements=[])\n",
      "                 ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-build-env-ola19u55\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 301, in _get_build_requires\n",
      "          self.run_setup()\n",
      "          ~~~~~~~~~~~~~~^^\n",
      "        File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-build-env-ola19u55\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 512, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "          ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\pip-build-env-ola19u55\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "          exec(code, locals())\n",
      "          ~~~~^^^^^^^^^^^^^^^^\n",
      "        File \"<string>\", line 128, in <module>\n",
      "        File \"C:\\Python313\\Lib\\subprocess.py\", line 414, in check_call\n",
      "          retcode = call(*popenargs, **kwargs)\n",
      "        File \"C:\\Python313\\Lib\\subprocess.py\", line 395, in call\n",
      "          with Popen(*popenargs, **kwargs) as p:\n",
      "               ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Python313\\Lib\\subprocess.py\", line 1036, in __init__\n",
      "          self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "          ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                              pass_fds, cwd, env,\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "          ...<5 lines>...\n",
      "                              gid, gids, uid, umask,\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                              start_new_session, process_group)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Python313\\Lib\\subprocess.py\", line 1548, in _execute_child\n",
      "          hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                             ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                                   # no special security\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "          ...<4 lines>...\n",
      "                                   cwd,\n",
      "                                   ^^^^\n",
      "                                   startupinfo)\n",
      "                                   ^^^^^^^^^^^^\n",
      "      FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch sentencepiece -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load configuration, tokenizer, and model\n",
    "\n",
    "- Load training configuration from a YAML file.\n",
    "- Initialize tokenizer based on the pretrained model path.\n",
    "- Load the pretrained causal language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myaml\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load config\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load config\n",
    "with open('SkyReach-AI-Bilingual-Training\\train_config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['tokenizer_path'])\n",
    "model = AutoModelForCausalLM.from_pretrained(config['model_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and preprocess dataset\n",
    "\n",
    "- Load bilingual dataset from JSON files.\n",
    "- Define a tokenization function to truncate and limit sequence length to 512 tokens.\n",
    "- Apply tokenization to dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m'\u001b[39m, data_files=config[\u001b[33m'\u001b[39m\u001b[33mdataset_path\u001b[39m\u001b[33m'\u001b[39m])[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Tokenize dataset\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize_fn\u001b[39m(example):\n",
      "\u001b[31mNameError\u001b[39m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('json', data_files=config['dataset_path'])['train']\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example['text'], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup training parameters\n",
    "\n",
    "- Define training hyperparameters like epochs, batch size, and learning rate.\n",
    "- Configure logging and checkpoint saving frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results\",\n",
    "    num_train_epochs=config['epochs'],\n",
    "    per_device_train_batch_size=config['batch_size'],\n",
    "    learning_rate=config['learning_rate'],\n",
    "    logging_dir='../logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize Trainer and start training\n",
    "\n",
    "- Instantiate the Hugging Face `Trainer` with model, args, and dataset.\n",
    "- Begin fine-tuning the bilingual model on the tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
